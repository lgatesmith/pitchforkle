{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501963b3",
   "metadata": {},
   "source": [
    "### Import Statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2475b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "import brotli\n",
    "import csv\n",
    "import html\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "from pathlib import PurePath\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d9d7a",
   "metadata": {},
   "source": [
    "### Defining Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c30a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_random_time(min_time, max_time):\n",
    "    \"\"\"Waits for a random amount of time between min_time and max_time (inclusive).\"\"\"\n",
    "    sleep_time = random.uniform(min_time, max_time)\n",
    "    print(f\"Waiting random amount of seconds ({round(sleep_time,2)}s in this case) before continuing...\")\n",
    "    time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b4caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(list_length, index):\n",
    "    if list_length == 96:         # Most times, each page of https://pitchfork.com/reviews/albums/?page=x\n",
    "                                  # has 96 albums...\n",
    "        if index == 12:\n",
    "            print(\"~\\n█░░░░░░░ 12.5%\")\n",
    "        elif index == 24:\n",
    "            print(\"~\\n███░░░░░░░░░ 25%\")\n",
    "        elif index == 36:\n",
    "            print(\"~\\n████░░░░░░░ 37%.5\")\n",
    "        elif index == 48:\n",
    "            print(\"~\\n██████░░░░░░ 50%\")\n",
    "        elif index == 60:\n",
    "            print(\"~\\n████████░░░░░ 62.5%\")\n",
    "        elif index == 72:\n",
    "            print(\"~\\n█████████░░░ 75%\")\n",
    "        elif index == 84:\n",
    "            print(\"~\\n█████████▓░ 87.5%\")\n",
    "            \n",
    "    if list_length == 190:      # However, one request I made showed 190 albums per page. This might have been\n",
    "                                # a fluke (it was around the time P4k added new reviews to their site... I also\n",
    "                                # got a 502 Bad Gateway error around this time)\n",
    "        if index == 19:\n",
    "            print(\"~\\n█░░░░░░░░░ 10%\")\n",
    "        elif index == 38:\n",
    "            print(\"~\\n██░░░░░░░░ 20%\")\n",
    "        elif index == 57:\n",
    "            print(\"~\\n███░░░░░░░ 30%\")\n",
    "        elif index == 76:\n",
    "            print(\"~\\n████░░░░░░ 40%\")\n",
    "        elif index == 95:\n",
    "            print(\"~\\n██████░░░░░░ 50%\")\n",
    "        elif index == 114:\n",
    "            print(\"~\\n██████░░░░ 60%\")\n",
    "        elif index == 133:\n",
    "            print(\"~\\n███████░░░ 70%\")\n",
    "        elif index == 152:\n",
    "            print(\"~\\n\t████████░░ 80%\")\n",
    "        elif index == 171:\n",
    "            print(\"~\\n█████████░ 90%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e210cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchfork_pg_x = r'https://pitchfork.com/reviews/albums/?page=' + f'{4}'\n",
    "req = urllib.request.Request(pitchfork_pg_x) # Constructing HTTP request obj\n",
    "with urllib.request.urlopen(req) as page_response:\n",
    "    r = page_response.read()\n",
    "    bebe_decoded = r.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944cd446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_code = page_response.code\n",
    "status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e34ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RequestAlbumsPage(page_number, song_dict_):\n",
    "    while True:\n",
    "        try:\n",
    "            wait_random_time(31,68)\n",
    "            pitchfork_pg_x = r'https://pitchfork.com/reviews/albums/?page=' + f'{page_number}'\n",
    "            print(f\"\\nRequesting page {page_number} of pitchfork.com/reviews/albums ...\\n\")\n",
    "            req = urllib.request.Request(pitchfork_pg_x) # Constructing HTTP request obj\n",
    "            with urllib.request.urlopen(req) as page_response:\n",
    "                http_status_code = page_response.code\n",
    "                r = page_response.read()\n",
    "                bebe_decoded = r.decode(\"utf-8\")\n",
    "    \n",
    "                # list split up by album text code things:\n",
    "                splitbythings = bebe_decoded.split('\"@type\":\"ListItem\",\"name\":')\n",
    "\n",
    "                clean_split_items = []\n",
    "\n",
    "                for i in range(len(splitbythings)):\n",
    "                    if i == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        itersplit_dirty = splitbythings[i]\n",
    "                        itersplit_1 = itersplit_dirty.split(',\"position\"')[0]\n",
    "                        if r\"\\u002Freviews\\u002Falbums\" not in itersplit_1:\n",
    "                            clean_split_items.append([itersplit_1, itersplit_dirty])\n",
    "\n",
    "                for item in clean_split_items:\n",
    "                    clean = item[0]\n",
    "                    dirty = item[1]\n",
    "                    firstsplit = clean.split('\"*')[1]\n",
    "                    albumname_rough_parse = firstsplit.split('\",\"url\":\"')[0]\n",
    "                    secondsplit = firstsplit.split('\",\"url\":\"')[1]\n",
    "                    url_parsed = secondsplit.split('/\"')[0]\n",
    "\n",
    "                    if albumname_rough_parse[-1:] == '*':\n",
    "                         albumname_clean = albumname_rough_parse[:-1]\n",
    "                    elif albumname_rough_parse[-1:] == 'P':\n",
    "                        albumname_clean = albumname_rough_parse[:-4] + \" <EP>\"\n",
    "                    else:\n",
    "                        print(\"Woah buddy, you haven't accounted for this case!\")\n",
    "\n",
    "                    url_purepath = PurePath(url_parsed)\n",
    "                    url_songdesc = url_purepath.name\n",
    "\n",
    "                    song_dict_[url_songdesc]['album_name'] = albumname_clean\n",
    "                    song_dict_[url_songdesc]['review_url'] = url_parsed\n",
    "\n",
    "\n",
    "                    song_split_str = url_songdesc + '\"},\"showAssetOnly\"'\n",
    "                    artistname_split_1 = itersplit_dirty.split(song_split_str)\n",
    "                    artistname_split_2 = artistname_split_1[0].split('\"subHed\":{\"name\":\"')[-1]\n",
    "                    artist_name_parsed = artistname_split_2.split('\",\"url\":')[0]\n",
    "\n",
    "                    song_dict_[url_songdesc]['artist_name'] = artist_name_parsed\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "        except urllib.error.HTTPError as e:\n",
    "            # Handle specific non-200 HTTP errors\n",
    "            print(f\"HTTP Error: Status code {e.code}. Retrying in {delay} seconds...\")\n",
    "        except urllib.error.URLError as e:\n",
    "            # Handle other URL errors (e.g., connection issues, invalid URL)\n",
    "            print(f\"URL Error: {e.reason}. Retrying in {delay} seconds...\")\n",
    "        except Exception as e:\n",
    "            # Handle any other unexpected exceptions\n",
    "            print(f\"An unexpected error occurred: {e}. Retrying in {delay} seconds...\")\n",
    "            \n",
    "            \n",
    "            \n",
    "def RequestIndividualReviewPage(page_number, song_dict_):\n",
    "    while True:\n",
    "        try:\n",
    "            wait_random_time(31,68)\n",
    "            song_dict_reg = dict(song_dict_)\n",
    "            for index, key_string in enumerate(song_dict_reg):\n",
    "\n",
    "                progress_bar(len(song_dict_reg), index)\n",
    "\n",
    "                individual_review_url = song_dict_reg[key_string]['review_url']\n",
    "                print(f\"~\\nPrepping to request {key_string} ({index+1}/{len(song_dict_reg)} - page {page_number})...\")\n",
    "                req = urllib.request.Request(individual_review_url)\n",
    "                r = urllib.request.urlopen(req).read()\n",
    "                jeen_decoded = r.decode(\"utf-8\")\n",
    "\n",
    "                infoslicefields_split1 = jeen_decoded.split(',\"infoSliceFields\":')[1]\n",
    "                infoslicefields_split2 = infoslicefields_split1.split(',\"isMusicReview\":')\n",
    "                infoslicefields = json.loads(infoslicefields_split2[0])\n",
    "\n",
    "                genre_parsed = infoslicefields['genre']\n",
    "                song_dict_reg[key_string]['genre'] = genre_parsed\n",
    "\n",
    "                try:\n",
    "                    label_parsed = infoslicefields['label']\n",
    "                    song_dict_reg[key_string]['label'] = label_parsed\n",
    "                except:\n",
    "                    song_dict_reg[key_string]['label'] = \"\"\n",
    "\n",
    "                reviewDate_parsed = infoslicefields['reviewDate']\n",
    "                song_dict_reg[key_string]['review_date'] = reviewDate_parsed\n",
    "\n",
    "                releaseYear_parsed = infoslicefields['releaseYear']\n",
    "                song_dict_reg[key_string]['release_date'] = releaseYear_parsed\n",
    "\n",
    "                nextpart_split1 = infoslicefields_split1.split('},\"isMusicReview\":')[1]\n",
    "                nextpart_split2 = nextpart_split1.split(',\"lede\":{')[0]\n",
    "                nextpart_split3 = nextpart_split2.split(',\"isTrackReview\":')\n",
    "                isMusicReview_parsed = nextpart_split3[0]\n",
    "                song_dict_reg[key_string]['is_music_review'] = isMusicReview_parsed\n",
    "\n",
    "                nextpart_split4 = nextpart_split3[1].split(',\"musicRating\":')\n",
    "                isTrackReview_parsed = nextpart_split4[0]\n",
    "                song_dict_reg[key_string]['is_track_review'] = isTrackReview_parsed\n",
    "\n",
    "                nextpart_split5 = nextpart_split4[1].split(',\"modifiedDate\":\"')\n",
    "                lildict = json.loads(nextpart_split5[0])\n",
    "                isBestNewMusic_parsed = lildict['isBestNewMusic']\n",
    "                song_dict_reg[key_string]['is_best_new_music'] = isBestNewMusic_parsed\n",
    "\n",
    "                isBestNewReissue_parsed = lildict['isBestNewReissue']\n",
    "                song_dict_reg[key_string]['is_best_new_reissue'] = isBestNewReissue_parsed\n",
    "\n",
    "                score_parsed = lildict['score']\n",
    "                song_dict_reg[key_string]['rating'] = score_parsed\n",
    "\n",
    "                authorname_split1 = jeen_decoded.split('><meta name=\"author\" content=\"')\n",
    "                authorname_parsed = authorname_split1[1].split('\"/><meta name=')[0]\n",
    "                song_dict_reg[key_string]['review_author_name'] = authorname_parsed\n",
    "\n",
    "                descrip_split1 = jeen_decoded.split('><meta name=\"description\" content=\"')[1]\n",
    "                descrip_parsed = descrip_split1.split('\"/><meta name=\"id\" content=\"')[0]\n",
    "                song_dict_reg[key_string]['review_description'] = descrip_parsed\n",
    "\n",
    "                content_id_split1 = jeen_decoded.split('><meta name=\"id\" content=\"')[1]\n",
    "                content_id_parsed = content_id_split1.split('\"/><meta name=\"keywords\" content=\"')[0]\n",
    "                song_dict_reg[key_string]['content_id'] = content_id_parsed\n",
    "\n",
    "                photo_id_split1 = jeen_decoded.split(r'https://media.pitchfork.com/photos/')[1]\n",
    "                photo_id_parsed = photo_id_split1.split(r'/16:9/w_1000,c_limit/')[0]\n",
    "                song_dict_reg[key_string]['photo_id'] = photo_id_parsed\n",
    "\n",
    "                full_photo_url = r'https://media.pitchfork.com/photos/' + photo_id_parsed + r'/master/w_1280,c_limit'\n",
    "                song_dict_reg[key_string]['full_photo_url'] = full_photo_url\n",
    "\n",
    "            print(\"██████████ 100% -- COMPLETE\")\n",
    "            return song_dict_reg\n",
    "        \n",
    "        except urllib.error.HTTPError as e:\n",
    "            # Handle specific non-200 HTTP errors\n",
    "            print(f\"HTTP Error: Status code {e.code}. Retrying in {delay} seconds...\")\n",
    "        except urllib.error.URLError as e:\n",
    "            # Handle other URL errors (e.g., connection issues, invalid URL)\n",
    "            print(f\"URL Error: {e.reason}. Retrying in {delay} seconds...\")\n",
    "        except Exception as e:\n",
    "            # Handle any other unexpected exceptions\n",
    "            print(f\"An unexpected error occurred: {e}. Retrying in {delay} seconds...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3857f36c",
   "metadata": {},
   "source": [
    "# Running the script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc830541",
   "metadata": {},
   "source": [
    "#### Create empty dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d6edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydicto = {}\n",
    "mydicto = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64d1ce",
   "metadata": {},
   "source": [
    "#### Enter in page of pitchfork.com/reviews/album you will be scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb4d13be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What album page are you scraping? Type in just the page number integer: 4\n",
      " \n",
      "Waiting random amount of seconds (41.74s in this case) before continuing...\n",
      "\n",
      "Requesting page 4 of pitchfork.com/reviews/albums ...\n",
      "\n",
      "~\n",
      "Prepping to request steve-gunn-music-for-writers (1/96 - page 4)...\n",
      "Waiting random amount of seconds (42.66s in this case) before continuing...\n",
      "~\n",
      "Prepping to request racing-mount-pleasant-racing-mount-pleasant (2/96 - page 4)...\n",
      "Waiting random amount of seconds (64.18s in this case) before continuing...\n",
      "~\n",
      "Prepping to request dijon-baby (3/96 - page 4)...\n",
      "Waiting random amount of seconds (40.04s in this case) before continuing...\n",
      "~\n",
      "Prepping to request pool-kids-easier-said-than-done (4/96 - page 4)...\n",
      "Waiting random amount of seconds (55.92s in this case) before continuing...\n",
      "~\n",
      "Prepping to request rio-da-yung-og-flint-feeling-like-im-not-through (5/96 - page 4)...\n",
      "Waiting random amount of seconds (32.03s in this case) before continuing...\n",
      "~\n",
      "Prepping to request various-artists-songs-in-the-key-of-z (6/96 - page 4)...\n",
      "Waiting random amount of seconds (42.81s in this case) before continuing...\n",
      "~\n",
      "Prepping to request cass-mccombs-interior-live-oak (7/96 - page 4)...\n",
      "Waiting random amount of seconds (38.5s in this case) before continuing...\n",
      "~\n",
      "Prepping to request radiohead-hail-to-the-thief-live-recordings-2003-2009 (8/96 - page 4)...\n",
      "Waiting random amount of seconds (51.29s in this case) before continuing...\n",
      "~\n",
      "Prepping to request pile-sunshine-and-balance-beams (9/96 - page 4)...\n",
      "Waiting random amount of seconds (36.61s in this case) before continuing...\n",
      "~\n",
      "Prepping to request mondo-lava-utero-dei (10/96 - page 4)...\n",
      "Waiting random amount of seconds (67.49s in this case) before continuing...\n",
      "~\n",
      "Prepping to request mgk-lost-americana (11/96 - page 4)...\n",
      "Waiting random amount of seconds (45.76s in this case) before continuing...\n",
      "~\n",
      "Prepping to request phil-elverum-arrington-de-dionyso-giant-opening-mouth-on-the-ground (12/96 - page 4)...\n",
      "Waiting random amount of seconds (61.21s in this case) before continuing...\n",
      "~\n",
      "█░░░░░░░ 12.5%\n",
      "~\n",
      "Prepping to request wolfacejoeyy-summersongs (13/96 - page 4)...\n",
      "Waiting random amount of seconds (41.09s in this case) before continuing...\n",
      "~\n",
      "Prepping to request quadeca-vanisher-horizon-scraper (14/96 - page 4)...\n",
      "Waiting random amount of seconds (46.32s in this case) before continuing...\n",
      "~\n",
      "Prepping to request coatshek-sound-bath (15/96 - page 4)...\n",
      "Waiting random amount of seconds (61.27s in this case) before continuing...\n",
      "~\n",
      "Prepping to request field-mic-surrender-instead (16/96 - page 4)...\n",
      "Waiting random amount of seconds (39.9s in this case) before continuing...\n",
      "~\n",
      "Prepping to request osees-abomination-revealed-at-last (17/96 - page 4)...\n",
      "Waiting random amount of seconds (31.73s in this case) before continuing...\n",
      "~\n",
      "Prepping to request anamanaguchi-anyway (18/96 - page 4)...\n",
      "Waiting random amount of seconds (36.7s in this case) before continuing...\n",
      "~\n",
      "Prepping to request recoechi-flavaz (19/96 - page 4)...\n",
      "Waiting random amount of seconds (44.72s in this case) before continuing...\n",
      "~\n",
      "Prepping to request no-joy-bugland (20/96 - page 4)...\n",
      "Waiting random amount of seconds (55.35s in this case) before continuing...\n",
      "~\n",
      "Prepping to request big-freedia-pressing-onward (21/96 - page 4)...\n",
      "Waiting random amount of seconds (65.14s in this case) before continuing...\n",
      "~\n",
      "Prepping to request vylet-pony-love-and-ponystep (22/96 - page 4)...\n",
      "Waiting random amount of seconds (57.26s in this case) before continuing...\n",
      "~\n",
      "Prepping to request toni-braxton-secrets (23/96 - page 4)...\n",
      "Waiting random amount of seconds (44.04s in this case) before continuing...\n",
      "~\n",
      "Prepping to request nick-drake-the-making-of-five-leaves-left (24/96 - page 4)...\n",
      "Waiting random amount of seconds (58.42s in this case) before continuing...\n",
      "~\n",
      "███░░░░░░░░░ 25%\n",
      "~\n",
      "Prepping to request amaarae-black-star (25/96 - page 4)...\n",
      "Waiting random amount of seconds (40.43s in this case) before continuing...\n",
      "~\n",
      "Prepping to request ada-lea-when-i-paint-my-masterpiece (26/96 - page 4)...\n",
      "Waiting random amount of seconds (56.94s in this case) before continuing...\n",
      "~\n",
      "Prepping to request mal-blum-the-villain (27/96 - page 4)...\n",
      "Waiting random amount of seconds (62.47s in this case) before continuing...\n",
      "~\n",
      "Prepping to request ethel-cain-willoughby-tucker-ill-always-love-you (28/96 - page 4)...\n",
      "Waiting random amount of seconds (44.64s in this case) before continuing...\n",
      "~\n",
      "Prepping to request the-black-keys-no-rain-no-flowers (29/96 - page 4)...\n",
      "Waiting random amount of seconds (40.18s in this case) before continuing...\n",
      "~\n",
      "Prepping to request ali-sethi-love-language (30/96 - page 4)...\n",
      "Waiting random amount of seconds (42.48s in this case) before continuing...\n",
      "~\n",
      "Prepping to request metro-boomin-presents-a-futuristic-summa-hosted-by-dj-spinz (31/96 - page 4)...\n",
      "Waiting random amount of seconds (58.82s in this case) before continuing...\n",
      "~\n",
      "Prepping to request dj-k-radio-libertadora (32/96 - page 4)...\n",
      "Waiting random amount of seconds (41.81s in this case) before continuing...\n",
      "~\n",
      "Prepping to request xaviersobased-once-more-ep (33/96 - page 4)...\n",
      "Waiting random amount of seconds (56.51s in this case) before continuing...\n",
      "~\n",
      "Prepping to request heatmiser-mic-city-sons-30th-anniversary (34/96 - page 4)...\n",
      "Waiting random amount of seconds (66.27s in this case) before continuing...\n",
      "~\n",
      "Prepping to request araabmuzik-electronic-dream-2 (35/96 - page 4)...\n",
      "Waiting random amount of seconds (51.91s in this case) before continuing...\n",
      "~\n",
      "Prepping to request 2pillz-pillzcasso (36/96 - page 4)...\n",
      "Waiting random amount of seconds (37.94s in this case) before continuing...\n",
      "~\n",
      "████░░░░░░░ 37%.5\n",
      "~\n",
      "Prepping to request the-armed-the-future-is-here-and-everything-needs-to-be-destroyed (37/96 - page 4)...\n",
      "Waiting random amount of seconds (59.22s in this case) before continuing...\n",
      "~\n",
      "Prepping to request rounak-maiti-brute-fact-home-truth (38/96 - page 4)...\n",
      "Waiting random amount of seconds (62.97s in this case) before continuing...\n",
      "~\n",
      "Prepping to request tommy-genesis-genesis (39/96 - page 4)...\n",
      "Waiting random amount of seconds (52.81s in this case) before continuing...\n",
      "~\n",
      "Prepping to request shudder-to-think-pony-express-record (40/96 - page 4)...\n",
      "Waiting random amount of seconds (54.72s in this case) before continuing...\n",
      "~\n",
      "Prepping to request renee-rapp-bite-me (41/96 - page 4)...\n",
      "Waiting random amount of seconds (59.31s in this case) before continuing...\n",
      "~\n",
      "Prepping to request wisp-if-not-winter (42/96 - page 4)...\n",
      "Waiting random amount of seconds (64.12s in this case) before continuing...\n",
      "~\n",
      "Prepping to request sofia-kourtesis-volver-ep (43/96 - page 4)...\n",
      "Waiting random amount of seconds (53.82s in this case) before continuing...\n",
      "~\n",
      "Prepping to request zayallcaps-art-pop-pop-art (44/96 - page 4)...\n",
      "Waiting random amount of seconds (67.25s in this case) before continuing...\n",
      "~\n",
      "Prepping to request freddie-gibbs-the-alchemist-alfredo-2 (45/96 - page 4)...\n",
      "Waiting random amount of seconds (66.58s in this case) before continuing...\n",
      "~\n",
      "Prepping to request youngboy-never-broke-again-masa (46/96 - page 4)...\n",
      "Waiting random amount of seconds (48.02s in this case) before continuing...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 500: Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m current_albums_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat album page are you scraping? Type in just the page number integer: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m myreturn \u001b[38;5;241m=\u001b[39m RequestAlbumsPage(current_albums_page, mydicto)\n",
      "Cell \u001b[1;32mIn[6], line 61\u001b[0m, in \u001b[0;36mRequestAlbumsPage\u001b[1;34m(page_number, song_dict_)\u001b[0m\n\u001b[0;32m     59\u001b[0m wait_random_time(\u001b[38;5;241m31\u001b[39m,\u001b[38;5;241m68\u001b[39m)\n\u001b[0;32m     60\u001b[0m req \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(individual_review_url)\n\u001b[1;32m---> 61\u001b[0m r \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(req)\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     62\u001b[0m jeen_decoded \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m infoslicefields_split1 \u001b[38;5;241m=\u001b[39m jeen_decoded\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfoSliceFields\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:557\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    555\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    556\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 557\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:749\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    746\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    747\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mopen(new, timeout\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 500: Internal Server Error"
     ]
    }
   ],
   "source": [
    "current_albums_page = input(\"What album page are you scraping? Type in just the page number integer: \")\n",
    "print(\" \")\n",
    "RequestAlbumsPage(current_albums_page, mydicto)\n",
    "myreturn = RequestIndividualReviewPage(page_number, song_dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc51cf2",
   "metadata": {},
   "source": [
    "#### Save returned dictionary to a csv (enter in csv name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551da8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(myreturn, orient='index')\n",
    "df.index.name = 'Album_Key'\n",
    "\n",
    "datafolder = r'C:\\Users\\michael.felzan\\Desktop\\Misc\\Pitchforkle\\Data'\n",
    "\n",
    "output_csv = os.path.join(datafolder, f\"page{current_albums_page}_12192025.csv\") # replace with auto-datetime function\n",
    "df.to_csv(output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985b25c",
   "metadata": {},
   "source": [
    "#### Cycle through returned dictionary of song info, request and save album thumbnails for each review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23098fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datafolder = r'C:\\Users\\michael.felzan\\Desktop\\Misc\\Pitchforkle\\Data\\Album_Art'\n",
    "\n",
    "for index, key_ in enumerate(myreturn):\n",
    "    progress_bar(len(myreturn), index)\n",
    "    image_url = myreturn[key_]['full_photo_url']\n",
    "    image_name = key_ + \".png\"\n",
    "    wait_random_time(9,21)\n",
    "    img_data = requests.get(image_url).content\n",
    "    image_output = os.path.join(datafolder, image_name)\n",
    "    with open(image_output, 'wb') as handler:\n",
    "        handler.write(img_data)\n",
    "        print(f\"saved {key_}.png ({index+1}/{len(myreturn)})\\n~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f9f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 1% bar\n",
    "# Combine album image scraping code with review info scraping code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc08ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834cc06f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3122f1e7",
   "metadata": {},
   "source": [
    "### The following code is for debugging / testing new features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "individual_review_url = r'https://pitchfork.com/reviews/albums/yungmorpheus-dirty-art-club-a-spyglass-to-ones-face/'\n",
    "req = urllib.request.Request(individual_review_url)\n",
    "r = urllib.request.urlopen(req).read()\n",
    "jeen_decoded = r.decode(\"utf-8\")\n",
    "\n",
    "infoslicefields_split1 = jeen_decoded.split(',\"infoSliceFields\":')[1]\n",
    "infoslicefields_split2 = infoslicefields_split1.split(',\"isMusicReview\":')\n",
    "infoslicefields = json.loads(infoslicefields_split2[0])\n",
    "\n",
    "genre_parsed = infoslicefields['genre']\n",
    "#song_dict_reg[key_string]['genre'] = genre_parsed\n",
    "\n",
    "try:\n",
    "    label_parsed = infoslicefields['label']\n",
    "    #song_dict_reg[key_string]['label'] = label_parsed\n",
    "except:\n",
    "    pass\n",
    "\n",
    "reviewDate_parsed = infoslicefields['reviewDate']\n",
    "#song_dict_reg[key_string]['review_date'] = reviewDate_parsed\n",
    "\n",
    "releaseYear_parsed = infoslicefields['releaseYear']\n",
    "#song_dict_reg[key_string]['release_date'] = releaseYear_parsed\n",
    "\n",
    "nextpart_split1 = infoslicefields_split1.split('},\"isMusicReview\":')[1]\n",
    "nextpart_split2 = nextpart_split1.split(',\"lede\":{')[0]\n",
    "nextpart_split3 = nextpart_split2.split(',\"isTrackReview\":')\n",
    "isMusicReview_parsed = nextpart_split3[0]\n",
    "#song_dict_reg[key_string]['is_music_review'] = isMusicReview_parsed\n",
    "\n",
    "nextpart_split4 = nextpart_split3[1].split(',\"musicRating\":')\n",
    "isTrackReview_parsed = nextpart_split4[0]\n",
    "#song_dict_reg[key_string]['is_track_review'] = isTrackReview_parsed\n",
    "\n",
    "nextpart_split5 = nextpart_split4[1].split(',\"modifiedDate\":\"')\n",
    "lildict = json.loads(nextpart_split5[0])\n",
    "isBestNewMusic_parsed = lildict['isBestNewMusic']\n",
    "#song_dict_reg[key_string]['is_best_new_music'] = isBestNewMusic_parsed\n",
    "\n",
    "isBestNewReissue_parsed = lildict['isBestNewReissue']\n",
    "#song_dict_reg[key_string]['is_best_new_reissue'] = isBestNewReissue_parsed\n",
    "\n",
    "score_parsed = lildict['score']\n",
    "#song_dict_reg[key_string]['rating'] = score_parsed\n",
    "\n",
    "authorname_split1 = jeen_decoded.split('><meta name=\"author\" content=\"')\n",
    "authorname_parsed = authorname_split1[1].split('\"/><meta name=')[0]\n",
    "#song_dict_reg[key_string]['review_author_name'] = authorname_parsed\n",
    "\n",
    "descrip_split1 = jeen_decoded.split('><meta name=\"description\" content=\"')[1]\n",
    "descrip_parsed = descrip_split1.split('\"/><meta name=\"id\" content=\"')[0]\n",
    "#song_dict_reg[key_string]['review_description'] = descrip_parsed\n",
    "\n",
    "content_id_split1 = jeen_decoded.split('><meta name=\"id\" content=\"')[1]\n",
    "content_id_parsed = content_id_split1.split('\"/><meta name=\"keywords\" content=\"')[0]\n",
    "#song_dict_reg[key_string]['content_id'] = content_id_parsed\n",
    "\n",
    "photo_id_split1 = jeen_decoded.split(r'https://media.pitchfork.com/photos/')[1]\n",
    "photo_id_parsed = photo_id_split1.split(r'/16:9/w_1000,c_limit/')[0]\n",
    "#song_dict_reg[key_string]['photo_id'] = photo_id_parsed\n",
    "\n",
    "full_photo_url = r'https://media.pitchfork.com/photos/' + photo_id_parsed + r'/master/w_1280,c_limit'\n",
    "#song_dict_reg[key_string]['full_photo_url'] = full_photo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76811914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37360bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "waybackmachine_url = r'https://web.archive.org/web/20160310042721/http://pitchfork.com/reviews/albums/3/'\n",
    "\n",
    "req = urllib.request.Request(waybackmachine_url)\n",
    "r = urllib.request.urlopen(req).read()\n",
    "peen_decoded = r.decode(\"utf-8\")\n",
    "\n",
    "peen_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2b4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
